{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy import attrs\n",
    "from spacy.symbols import VERB, NOUN, ADV, ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/anais/anaconda3/lib/python36.zip', '/Users/anais/anaconda3/lib/python3.6', '/Users/anais/anaconda3/lib/python3.6/lib-dynload', '/Users/anais/anaconda3/lib/python3.6/site-packages', '/Users/anais/anaconda3/lib/python3.6/site-packages/aeosa', '/Users/anais/anaconda3/lib/python3.6/site-packages/IPython/extensions', '/Users/anais/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'text'\n",
    "Y_COLUMN = 'author'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_pipeline(df, nlp_pipeline, pipeline_name=''):\n",
    "    y = df[Y_COLUMN].copy()\n",
    "    X = pd.Series(df[TEXT_COLUMN])\n",
    "    # If you've done EDA, you may have noticed that the author classes aren't quite balanced.\n",
    "    # We'll use stratified splits just to be on the safe side.\n",
    "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n",
    "    losses = []\n",
    "    for train_index, test_index in rskf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        nlp_pipeline.fit(X_train, y_train)\n",
    "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(X_test)))\n",
    "    print(f'{pipeline_name} kfolds log losses: {str([str(round(x, 3)) for x in sorted(losses)])}')\n",
    "    print(f'{pipeline_name} mean log loss: {round(pd.np.mean(losses), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\", encoding = 'latin-1', usecols=[TEXT_COLUMN, Y_COLUMN])\n",
    "test_df = pd.read_csv(\"test.csv\", encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams only kfolds log losses: ['0.741', '0.861', '0.9', '0.983', '1.003']\n",
      "Unigrams only mean log loss: 0.898\n"
     ]
    }
   ],
   "source": [
    "unigram_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "                        ])\n",
    "test_pipeline(train_df, unigram_pipe, \"Unigrams only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramPredictions(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.unigram_mnb = Pipeline([('text', CountVectorizer()), ('mnb', MultinomialNB())])\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # Every custom transformer requires a fit method. In this case, we want to train\n",
    "        # the naive bayes model.\n",
    "        self.unigram_mnb.fit(x, y)\n",
    "        return self\n",
    "    \n",
    "    def add_unigram_predictions(self, text_series):\n",
    "        # Resetting the index ensures the indexes equal the row numbers.\n",
    "        # This guarantees nothing will be misaligned when we merge the dataframes further down.\n",
    "        df = pd.DataFrame(text_series.reset_index(drop=True))\n",
    "        # Make unigram predicted probabilities and label them with the prediction class, aka \n",
    "        # the author.\n",
    "        unigram_predictions = pd.DataFrame(\n",
    "            self.unigram_mnb.predict_proba(text_series),\n",
    "            columns=['naive_bayes_pred_' + x for x in self.unigram_mnb.classes_]\n",
    "                                           )\n",
    "        # We only need 2 out of 3 columns, as the last is always one minus the \n",
    "        # sum of the other two. In some cases, that colinearity can actually be problematic.\n",
    "        del unigram_predictions[unigram_predictions.columns[0]]\n",
    "        df = df.merge(unigram_predictions, left_index=True, right_index=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, text_series):\n",
    "        # Every custom transformer also requires a transform method. This time we just want to \n",
    "        # provide the unigram predictions.\n",
    "        return self.add_unigram_predictions(text_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PartOfSpeechFeatures(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.NLP = NLP\n",
    "        # Store the number of cpus available for when we do multithreading later on\n",
    "        self.num_cores = cpu_count()\n",
    "\n",
    "    def part_of_speechiness(self, pos_counts, part_of_speech):\n",
    "        if eval(part_of_speech) in pos_counts:\n",
    "            return pos_counts[eval(part_of_speech).numerator]\n",
    "        return 0\n",
    "\n",
    "    def add_pos_features(self, df):\n",
    "        text_series = df[TEXT_COLUMN]\n",
    "        \"\"\"\n",
    "        Parse each sentence with part of speech tags. \n",
    "        Using spaCy's pipe method gives us multi-threading 'for free'. \n",
    "        This is important as this is by far the single slowest step in the pipeline.\n",
    "        If you want to test this for yourself, you can use:\n",
    "            from time import time \n",
    "            start_time = time()\n",
    "            (some code)\n",
    "            print(f'Code took {time() - start_time} seconds')\n",
    "        For faster functions the timeit module would be standard... but that's\n",
    "        meant for situations where you can wait for the function to be called 1,000 times.\n",
    "        \"\"\"\n",
    "        df['doc'] = [i for i in self.NLP.pipe(text_series.values, n_threads=self.num_cores)]\n",
    "        df['pos_counts'] = df['doc'].apply(lambda x: x.count_by(attrs.POS))\n",
    "        # We get a very minor speed boost here by using pandas built in string methods\n",
    "        # instead of df['doc'].apply(len). String processing is generally slow in python,\n",
    "        # use the pandas string methods directly where possible.\n",
    "        df['sentence_length'] = df['doc'].str.len()\n",
    "        # This next step generates the fraction of each sentence that is composed of a \n",
    "        # specific part of speech.\n",
    "        # There's admittedly some voodoo in this step. Math can be more highly optimized in python\n",
    "        # than string processing, so spaCy really stores the parts of speech as numbers. If you\n",
    "        # try >>> VERB in the console you'll get 98 as the result.\n",
    "        # The monkey business with eval() here allows us to generate several named columns\n",
    "        # without specifying in advance that {'VERB': 98}.\n",
    "        for part_of_speech in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
    "            df[f'{part_of_speech.lower()}iness'] = df['pos_counts'].apply(\n",
    "                lambda x: self.part_of_speechiness(x, part_of_speech))\n",
    "            df[f'{part_of_speech.lower()}iness'] /= df['sentence_length']\n",
    "        df['avg_word_length'] = (df['doc'].apply(\n",
    "            lambda x: sum([len(word) for word in x])) / df['sentence_length'])\n",
    "        return df\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        # since this transformer doesn't train a model, we don't actually need to do anything here.\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        return self.add_pos_features(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropStringColumns(TransformerMixin):\n",
    "    # You may have noticed something odd about this class: there's no __init__!\n",
    "    # It's actually inherited from TransformerMixin, so it doesn't need to be declared again.\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        for col, dtype in zip(df.columns, df.dtypes):\n",
    "            if dtype == object:\n",
    "                del df[col]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kfolds log losses: ['0.755', '0.885', '0.931', '0.946', '0.956']\n",
      " mean log loss: 0.895\n"
     ]
    }
   ],
   "source": [
    "logit_all_features_pipe = Pipeline([\n",
    "        ('uni', UnigramPredictions()),\n",
    "        ('nlp', PartOfSpeechFeatures()),\n",
    "        ('clean', DropStringColumns()), \n",
    "        ('clf', LogisticRegression())\n",
    "                                     ])\n",
    "test_pipeline(train_df, logit_all_features_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_submission_df(trained_prediction_pipeline, test_df):\n",
    "    predictions = pd.DataFrame(\n",
    "        trained_prediction_pipeline.predict_proba(test_df.text),\n",
    "        columns=trained_prediction_pipeline.classes_\n",
    "                               )\n",
    "    predictions['id'] = test_df['id']\n",
    "    predictions.to_csv(\"submission.csv\", index=False)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053537</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>0.918659</td>\n",
       "      <td>id02310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.932726</td>\n",
       "      <td>0.033733</td>\n",
       "      <td>0.033540</td>\n",
       "      <td>id24541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.058324</td>\n",
       "      <td>0.907781</td>\n",
       "      <td>0.033895</td>\n",
       "      <td>id00134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.926260</td>\n",
       "      <td>0.037840</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>id27757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058264</td>\n",
       "      <td>0.027316</td>\n",
       "      <td>0.914419</td>\n",
       "      <td>id04081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.032303</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>id27337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.894289</td>\n",
       "      <td>0.041626</td>\n",
       "      <td>0.064085</td>\n",
       "      <td>id24265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.063049</td>\n",
       "      <td>0.874148</td>\n",
       "      <td>0.062803</td>\n",
       "      <td>id25917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.939316</td>\n",
       "      <td>0.030851</td>\n",
       "      <td>0.029833</td>\n",
       "      <td>id04951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.838325</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.092675</td>\n",
       "      <td>id14549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.083965</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>0.905807</td>\n",
       "      <td>id22505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.746536</td>\n",
       "      <td>0.170863</td>\n",
       "      <td>id24002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.046702</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>0.943453</td>\n",
       "      <td>id18982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.742967</td>\n",
       "      <td>0.214007</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>id15181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.172478</td>\n",
       "      <td>0.586079</td>\n",
       "      <td>0.241444</td>\n",
       "      <td>id21888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.049837</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>0.940245</td>\n",
       "      <td>id12035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.767344</td>\n",
       "      <td>0.033059</td>\n",
       "      <td>0.199598</td>\n",
       "      <td>id17991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.904734</td>\n",
       "      <td>0.046935</td>\n",
       "      <td>0.048330</td>\n",
       "      <td>id10707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.906186</td>\n",
       "      <td>0.050256</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>id07101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.923697</td>\n",
       "      <td>0.032554</td>\n",
       "      <td>0.043749</td>\n",
       "      <td>id00345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.936129</td>\n",
       "      <td>0.029672</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>id05912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.222276</td>\n",
       "      <td>0.768915</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>id13443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.347186</td>\n",
       "      <td>0.631808</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>id09248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.078881</td>\n",
       "      <td>0.842872</td>\n",
       "      <td>0.078248</td>\n",
       "      <td>id17542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.808768</td>\n",
       "      <td>0.042128</td>\n",
       "      <td>0.149104</td>\n",
       "      <td>id06995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.848202</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.065279</td>\n",
       "      <td>id25159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.926779</td>\n",
       "      <td>0.034095</td>\n",
       "      <td>0.039126</td>\n",
       "      <td>id25729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.753136</td>\n",
       "      <td>0.183476</td>\n",
       "      <td>0.063388</td>\n",
       "      <td>id26949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.044113</td>\n",
       "      <td>0.008977</td>\n",
       "      <td>0.946910</td>\n",
       "      <td>id27191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.388057</td>\n",
       "      <td>0.020878</td>\n",
       "      <td>0.591065</td>\n",
       "      <td>id07668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.092705</td>\n",
       "      <td>0.083384</td>\n",
       "      <td>0.823911</td>\n",
       "      <td>id10030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.546087</td>\n",
       "      <td>0.240017</td>\n",
       "      <td>0.213896</td>\n",
       "      <td>id22952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.075278</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>0.893094</td>\n",
       "      <td>id21955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.468013</td>\n",
       "      <td>0.064974</td>\n",
       "      <td>0.467013</td>\n",
       "      <td>id07674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.740672</td>\n",
       "      <td>0.061185</td>\n",
       "      <td>0.198143</td>\n",
       "      <td>id13760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.917843</td>\n",
       "      <td>0.040145</td>\n",
       "      <td>0.042012</td>\n",
       "      <td>id18166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.059683</td>\n",
       "      <td>0.917615</td>\n",
       "      <td>0.022702</td>\n",
       "      <td>id23498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.902864</td>\n",
       "      <td>0.033640</td>\n",
       "      <td>0.063497</td>\n",
       "      <td>id05296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.416690</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.021386</td>\n",
       "      <td>id26709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.743549</td>\n",
       "      <td>0.217689</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>id02853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.465672</td>\n",
       "      <td>0.061363</td>\n",
       "      <td>0.472965</td>\n",
       "      <td>id06800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.922735</td>\n",
       "      <td>0.039409</td>\n",
       "      <td>0.037856</td>\n",
       "      <td>id14758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.935409</td>\n",
       "      <td>0.032331</td>\n",
       "      <td>0.032260</td>\n",
       "      <td>id12356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.897098</td>\n",
       "      <td>0.026856</td>\n",
       "      <td>0.076045</td>\n",
       "      <td>id21492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.954930</td>\n",
       "      <td>0.022829</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>id27540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.752392</td>\n",
       "      <td>0.035179</td>\n",
       "      <td>0.212429</td>\n",
       "      <td>id07376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.108650</td>\n",
       "      <td>0.267828</td>\n",
       "      <td>0.623522</td>\n",
       "      <td>id15201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.060754</td>\n",
       "      <td>0.924604</td>\n",
       "      <td>0.014642</td>\n",
       "      <td>id23388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.912025</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>0.045703</td>\n",
       "      <td>id13923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.072260</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.877975</td>\n",
       "      <td>id00794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.912396</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.048792</td>\n",
       "      <td>id23910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.044443</td>\n",
       "      <td>0.060798</td>\n",
       "      <td>0.894759</td>\n",
       "      <td>id02556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.762928</td>\n",
       "      <td>0.221492</td>\n",
       "      <td>0.015579</td>\n",
       "      <td>id22151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.142567</td>\n",
       "      <td>0.261065</td>\n",
       "      <td>0.596368</td>\n",
       "      <td>id11698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.156416</td>\n",
       "      <td>0.056469</td>\n",
       "      <td>0.787115</td>\n",
       "      <td>id11627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.803894</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.164310</td>\n",
       "      <td>id06340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.851644</td>\n",
       "      <td>0.037591</td>\n",
       "      <td>0.110764</td>\n",
       "      <td>id05123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.177115</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>0.807953</td>\n",
       "      <td>id26620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.553046</td>\n",
       "      <td>0.022943</td>\n",
       "      <td>0.424012</td>\n",
       "      <td>id04002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.059467</td>\n",
       "      <td>0.455994</td>\n",
       "      <td>0.484539</td>\n",
       "      <td>id26554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          EAP       HPL       MWS       id\n",
       "0    0.053537  0.027804  0.918659  id02310\n",
       "1    0.932726  0.033733  0.033540  id24541\n",
       "2    0.058324  0.907781  0.033895  id00134\n",
       "3    0.926260  0.037840  0.035900  id27757\n",
       "4    0.058264  0.027316  0.914419  id04081\n",
       "5    0.934884  0.032303  0.032813  id27337\n",
       "6    0.894289  0.041626  0.064085  id24265\n",
       "7    0.063049  0.874148  0.062803  id25917\n",
       "8    0.939316  0.030851  0.029833  id04951\n",
       "9    0.838325  0.069000  0.092675  id14549\n",
       "10   0.083965  0.010228  0.905807  id22505\n",
       "11   0.082600  0.746536  0.170863  id24002\n",
       "12   0.046702  0.009845  0.943453  id18982\n",
       "13   0.742967  0.214007  0.043027  id15181\n",
       "14   0.172478  0.586079  0.241444  id21888\n",
       "15   0.049837  0.009918  0.940245  id12035\n",
       "16   0.767344  0.033059  0.199598  id17991\n",
       "17   0.904734  0.046935  0.048330  id10707\n",
       "18   0.906186  0.050256  0.043558  id07101\n",
       "19   0.923697  0.032554  0.043749  id00345\n",
       "20   0.936129  0.029672  0.034198  id05912\n",
       "21   0.222276  0.768915  0.008809  id13443\n",
       "22   0.347186  0.631808  0.021007  id09248\n",
       "23   0.078881  0.842872  0.078248  id17542\n",
       "24   0.808768  0.042128  0.149104  id06995\n",
       "25   0.848202  0.086518  0.065279  id25159\n",
       "26   0.926779  0.034095  0.039126  id25729\n",
       "27   0.753136  0.183476  0.063388  id26949\n",
       "28   0.044113  0.008977  0.946910  id27191\n",
       "29   0.388057  0.020878  0.591065  id07668\n",
       "..        ...       ...       ...      ...\n",
       "469  0.092705  0.083384  0.823911  id10030\n",
       "470  0.546087  0.240017  0.213896  id22952\n",
       "471  0.075278  0.031627  0.893094  id21955\n",
       "472  0.468013  0.064974  0.467013  id07674\n",
       "473  0.740672  0.061185  0.198143  id13760\n",
       "474  0.917843  0.040145  0.042012  id18166\n",
       "475  0.059683  0.917615  0.022702  id23498\n",
       "476  0.902864  0.033640  0.063497  id05296\n",
       "477  0.416690  0.561924  0.021386  id26709\n",
       "478  0.743549  0.217689  0.038762  id02853\n",
       "479  0.465672  0.061363  0.472965  id06800\n",
       "480  0.922735  0.039409  0.037856  id14758\n",
       "481  0.935409  0.032331  0.032260  id12356\n",
       "482  0.897098  0.026856  0.076045  id21492\n",
       "483  0.954930  0.022829  0.022240  id27540\n",
       "484  0.752392  0.035179  0.212429  id07376\n",
       "485  0.108650  0.267828  0.623522  id15201\n",
       "486  0.060754  0.924604  0.014642  id23388\n",
       "487  0.912025  0.042272  0.045703  id13923\n",
       "488  0.072260  0.049765  0.877975  id00794\n",
       "489  0.912396  0.038812  0.048792  id23910\n",
       "490  0.044443  0.060798  0.894759  id02556\n",
       "491  0.762928  0.221492  0.015579  id22151\n",
       "492  0.142567  0.261065  0.596368  id11698\n",
       "493  0.156416  0.056469  0.787115  id11627\n",
       "494  0.803894  0.031795  0.164310  id06340\n",
       "495  0.851644  0.037591  0.110764  id05123\n",
       "496  0.177115  0.014932  0.807953  id26620\n",
       "497  0.553046  0.022943  0.424012  id04002\n",
       "498  0.059467  0.455994  0.484539  id26554\n",
       "\n",
       "[499 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_submission_df(logit_all_features_pipe, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
